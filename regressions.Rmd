---
title: "Regression Analysis for Categorical and Continuous Variables"
author: "Souvik Bhattacharya"
date: "3/8/2017"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Regression Analysis for Continuous variable.

First load the dataset and download the following packages. 

```{r}
install.packages("MASS")
install.packages("ggplot2")
install.packages("nlstools")
install.packages("nnet")

library(MASS)
data(cats)
```

View how the data looks like. 

```{r}
head(cats)
summary(cats)
```

"Bwt" is the body weight in kg, "Hwt" is the heart weight in gms, and "Sex‚Äù obvious. 

There are several ways of plotting the data. Lets try ggplot first. 

```{r}
plot(Hwt ~ Bwt, data=cats, xlab = "Body Weight", ylab = "Heart Weight")
library(ggplot2)

ggplot(cats, aes(x = Bwt, y = Hwt))+ geom_point()+xlab("Body Weight")+ylab("Heart Weight")
```

We can be interested to look into how the body weight compares with heart weight for each gender

```{r}
ggplot(cats, aes(x = Bwt, y = Hwt, group = "Sex"))+ geom_point()+facet_wrap(~Sex)+xlab("Body Weight")+ylab("Heart Weight")

```

The graph shows a moderate relationship between Heart and Body weight. But we need a strong mathematical quantity in order to understand the effect of one variable on the other. 

```{r}
with(cats, cor(Bwt, Hwt))
```

Alternatively you can try to use

```{r}
with(cats, cor.test(Bwt, Hwt))
```

Other scatter plots can be 

```{r}
with(cats, plot(Bwt, Hwt, type="n", las=1, xlab="Body Weight in kg", ylab="Heart Weight in g",main="Heart Weight vs. Body Weight of Cats"))
with(cats, points(Bwt[Sex=="F"], Hwt[Sex=="F"], pch=16, col="red"))
with(cats, points(Bwt[Sex=="M"], Hwt[Sex=="M"], pch=17, col="blue"))
```

With strong correlation, it makes sense to fit a linear model as it is the most simpler one to look into. 

```{r}
l1 = with(data = cats, lm(Bwt~Hwt))
summary(l1)
```

Predicted : Bwt = f(Hwt)Predict new data points 

```{r}
bwt = seq(0, 5, length = 200)
hwt = predict(l1, data.frame(Bwt = bwt))
plot(Hwt ~ Bwt, data=cats, xlab = "Body Weight", ylab = "Heart Weight", xlim = c(0,5))
points(bwt, hwt, type = "l", col = "red")
```

Since the data is scattered, a confidence interval can be obtained as well in the regression model



```{r}
hwt = predict(l1, data.frame(Bwt = bwt), interval = "confidence")
colnames(hwt) = c("HW", "Lower", "Upper")
newdata = data.frame(BW = bwt, hwt)
ggplot(newdata, aes(x = BW, y = HW))+geom_line(col ="red")+geom_ribbon(aes(ymin = Lower, ymax = Upper))+geom_point(data = cats, aes(x = Bwt, y = Hwt))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```
```{r}
ggplot(newdata, aes(x = BW, y = HW))+geom_line(col ="red")+geom_point(data = cats, aes(x = Bwt, y = Hwt))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```

For some data sets it make sense to try 2nd degree polynomial and plot solutions using plot command or ggplot

```{r}
l1 = with(data = cats, lm(Hwt~I(Bwt)+I(Bwt^2)))
bwt = seq(0, 5, length = 200)
hwt = predict(l1, data.frame(Bwt = bwt))
plot(Hwt ~ Bwt, data=cats, xlab = "Body Weight", ylab = "Heart Weight", xlim = c(0,5))
points(bwt, hwt, type = "l", col = "red")
```

```{r}
newdata = data.frame(BW = bwt, HW = hwt)
ggplot(newdata, aes(x = BW, y = HW))+geom_line(col ="red")+geom_point(data = cats, aes(x = Bwt, y = Hwt))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```

```{r}
bwt = seq(0, 5, length = 200)
hwt = predict(l1, data.frame(Bwt = bwt), interval = "confidence")
colnames(hwt) = c("HW", "Lower", "Upper")
newdata = data.frame(BW = bwt, hwt)
ggplot(newdata, aes(x = BW, y = HW))+geom_line(col ="red")+geom_ribbon(aes(ymin = Lower, ymax = Upper))+geom_point(data = cats, aes(x = Bwt, y = Hwt))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```
Or may be 3rd degree might be a better choice but completely depends on the structure of data
```{r}
l1 = with(data = cats, lm(Hwt~poly(Bwt,3)))
bwt = seq(0, 5, length = 200)
hwt = predict(l1, data.frame(Bwt = bwt))
plot(Hwt ~ Bwt, data=cats, xlab = "Body Weight", ylab = "Heart Weight", xlim = c(0,5))
points(bwt, hwt, type = "l", col = "red")
```

```{r}
newdata = data.frame(BW = bwt, HW = hwt)
ggplot(newdata, aes(x = BW, y = HW))+geom_line(col ="red")+geom_point(data = cats, aes(x = Bwt, y = Hwt))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```

```{r}
bwt = seq(0, 5, length = 200)
hwt = predict(l1, data.frame(Bwt = bwt), interval = "confidence")
colnames(hwt) = c("HW", "Lower", "Upper")
newdata = data.frame(BW = bwt, hwt)
ggplot(newdata, aes(x = BW, y = HW))+geom_line(col ="red")+geom_ribbon(aes(ymin = Lower, ymax = Upper))+geom_point(data = cats, aes(x = Bwt, y = Hwt))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```

We can also use regression to fit functions which are not linear or polynomial in nature. Any kind of function can be fitted using the library "nlstools". 

```{r}
library(nlstools)
l1 = nls(Hwt~fn(Bwt,a,b), cats, start = list(a=0,b=0.5),algorithm = "port")

hwt = predict(l1, data.frame(Bwt = bwt), interval = "confidence")
```

Confidence interval cannot be predicted using predict function like the linear models before. Hence a new approach using separate code is needed to obtain confidence interval. 

```{r}

Boo <- nlsBoot(l1, niter = 200)
Param_Boo <- Boo$coefboot

x <- bwt
curveDF <- data.frame(matrix(0,ncol = 2,nrow = 200*length(x)))

for(i in 1:200)
{
  for(j in 1:length(x))
  {
    # Function value
    curveDF[j+(i-1)*200,1] <- fn(x[j],Param_Boo[i,1],Param_Boo[i,2])
    # Bootstrap sample number
    curveDF[j+(i-1)*200,2] <- i
    # x value
    curveDF[j+(i-1)*200,3] <- x[j]
  }
  
}

colnames(curveDF) <- c('ys','bsP','xs')

cat = data.frame(cats, bsP = 1)

ggplot(data = curveDF, aes(x=xs, y=ys, group=bsP)) +geom_line(col = "yellow", alpha = 0.2) +geom_point(data = cat, aes(x = Bwt, y = Hwt))+ylim(c(0,20))+xlim(c(0,5))+xlab("Body Weight")+ ylab ("Heart Weight")
```


# Categorical Variable

For data which are categorical in narure, regression is not a good solution. Regression work best for continuous datasets. But for categorical variables, it is not a good solution. Hence a different approach using logistic regression is a better choice. 

```{r}
# Categorical 

x = seq(0,10, length = 100)
y = c(rep(0,50), rep(1,50))

plot(x,y, xlab = "Time of day", ylab = "Probability of Sleeping")
l1 = lm(y~x)
abline(l1)

library(nnet)

data = data.frame(x,y)
l1<- multinom(y~x, data)
l1<- multinom(y~x, data, maxit = 500)
l1<- multinom(y~x, data, maxit = 1000)
l1<- multinom(y~x, data, maxit = 100000)

predict(l1, data.frame(x = 5))
newtime = seq(0,10, by = 0.1)

a1 = as.numeric(coef(l1)[1])
b1 = as.numeric(coef(l1)[2])
fun = function(x){exp(a1 + b1*x)/(1+exp(a1 + b1*x))}



prob = fun(newtime)
plot(x,y, xlab = "Time of day", ylab = "Probability of Sleeping")
points(newtime, prob, type = "l", col = "red")



predict(l1, data.frame(x = 2))

prob = predict(l1, data.frame(x = newtime))
plot(x,y, xlab = "Time of day", ylab = "Probability of Sleeping")
points(newtime, prob, type = "l", col = "red")



prob = as.numeric(as.character(prob))
plot(x,y, xlab = "Time of day", ylab = "Probability of Sleeping")
points(newtime, prob, type = "l", col = "red")






ggplot(cats,aes(x = Bwt, y = Hwt, group = Sex))+geom_point(aes(color = Sex))

l1<- multinom(Sex~poly(Bwt*Hwt,2), cats)

datanew = data.frame(Bwt = rep(seq(1,4,length = 50),50), Hwt = rep(seq(5,21, length= 50), each = 50))
prob = predict(l1, data.frame(datanew))

preddata = data.frame(Sex = prob, datanew)

ggplot(preddata,aes(x = Bwt, y = Hwt, group = Sex))+geom_point(aes(color = Sex), pch = 1)+geom_point(data = cats,aes(x = Bwt, y = Hwt, color = Sex), pch = 19, lwd = 3)






